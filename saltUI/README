------------------------------------------------------------
  LAUI - Local Analytics User Interface, a tool for viewing
         and classifying flows collected from pcap2flow
------------------------------------------------------------


QUICK START FOR LAUI

# pull code from gitlab
#
git clone http://gitlab.cisco.com/mcgrew/json-flow-data-tools.git

# change to project directory
#
cd json-flow-data-tools/saltUI/

# start server
#
python server.py


TESTING (quick)

follow QUICK START instructions on your local machine

go to http://localhost:8080/home in Chrome/Firefox

click the Local Analytics drop down on the top banner

select Upload JSON File

click Browse, select test.json from the json-flow-data-tools/saltUI/
      folder

click Start Upload

results should then be displayed


TESTING (pcap2flow integration)

follow the pcap2flow instruction to begin a local capture

allow enough time for pcap2flow to generate json files

make sure that the output location of pcap2flow is the same as is
     specified in flocap.cfg (LAUI looks in flocap.cfg for the
     output dir)

from http://localhost:8080/home, click on "Click Here" on the line
     "For the most recent results, Click Here".

results should then be displayed


------------------------------------------------------------
LAUI CONFIG

All configuration for the LAUI user interface is located in "laui.cfg"

To configure which fields are displayed in the user interface, add the following line:

   display_field <json_field_name> <Human readable description> <Key into Metadata Struct>

All currently supported display_fields are listed in the configuration file. The human readable description needs to have whitespace replaced by '_'.  For example, the line to make source addresses visible in the UI is:

    display_field   sa      Source_Address  0

To configure which classifiers are run and displayed in the user interface, add the following line for logistic regression classifiers learned with model.py (see Learning Parameters section):

   classifier <Name> logreg <meta+SPLT parameter file> <meta+SPLT+Byte Disitribution parameter file>

The user interface uses a logistic regression model using just the metadata and SPLT data features if byte distribution does not exist, or the total number of bytes in the byte distribution is less than 100, otherwise the metadata + SPLT + byte distribution model is used.

There is also support for a "mapping" classifier which uses hard coded associations. These can be configured by added the following line:

      classifier <NAME> mapping <Key into Metadata Struct> <mapping file>

<Key into Metadata Struct> is the same as in the display_field. The mapping file is just a tab-delimited file with the values on the left and the mapping to a real-valued number between [0,1] with 1.0. For example, for client key lengths used in TLS communication, we could have the following file:

     512     1.0
     520     0.1
     768     0.9
     776     0.0
     1024    0.8
     2048    0.4
     3072    0.25
     4096    0.0
     _       -1.0

A 512-bit is very insecure, therefore it receives a value of 1.0 (higher is less secure in this model). The _ is used to score elements that do not exist in the mapping file. A working example of the client key lengths is:

     classifier  CKL        mapping   10      client_key_length.txt

------------------------------------------------------------
Learning Parameters

The model.py python program will generate logistic regression parameters suitable for use in the user interface. The arguments to model.py are:

  -p POS_DIR, --pos_dir POS_DIR
                        Directory of Positive Examples (JSON Format)
  -n NEG_DIR, --neg_dir NEG_DIR
                        Directory of Negative Examples (JSON Format)
  -m, --meta            Parse Metadata Information
  -l, --lengths         Parse Packet Size Information
  -t, --times           Parse Inter-packet Time Information
  -d, --dist            Parse Byte Distribution Information
  -o OUTPUT, --output OUTPUT
                        Output file for parameters

To generate the two parameter files to be used with LAUI, we first generate the parameter file that does use the byte distribution:

   python model.py -m -l -t -p /var/tls_json_files/ -n /var/non_tls_json_files/ -o params.txt

and then we generate the parameters that do use the byte distribution:

   python model.py -m -l -t -d -p /var/tls_json_files/ -n /var/non_tls_json_files/ -o params_bd.txt