#!/usr/bin/python

"""
joyq performs query operations on JSON-formatted flow objects; see joyq --help for more details

 *
 * Copyright (c) 2017 Cisco Systems, Inc.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 *
 *   Redistributions of source code must retain the above copyright
 *   notice, this list of conditions and the following disclaimer.
 *
 *   Redistributions in binary form must reproduce the above
 *   copyright notice, this list of conditions and the following
 *   disclaimer in the documentation and/or other materials provided
 *   with the distribution.
 *
 *   Neither the name of the Cisco Systems, Inc. nor the names of its
 *   contributors may be used to endorse or promote products derived
 *   from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
 * FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
 * COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,
 * INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
 * STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED
 * OF THE POSSIBILITY OF SUCH DAMAGE.
 *
"""

import sys
import gzip
import bz2
import copy
import collections
import os
import subprocess
import argparse
sys.path.append("/usr/local/lib/python")

try:
    from sleuth import *
except ImportError:
    # If in source code directory
    from sleuth_pkg import *

"""
Flow Iterator Classes
"""


class FlowIteratorFromFile(DictStreamIteratorFromFile):
    """
    Create a new DictIterator instance from the given input file.
    This allows iteration over all JSON objects within the file.
    """
    def __init__(self, file_name):
        self.pcap_loader = PcapLoader(file=file_name)
        super(FlowIteratorFromFile, self).__init__(file_name=file_name,
                                                   skip_lines=['version'])

    def _cleanup(self):
        """
        Overrides parent.
        Close any resources that are still open.
        :return:
        """
        try:
            self.f.close()
        except IOError:
            pass

        self.pcap_loader.cleanup()

    def _load_file(self):
        """
        Overrides parent.
        If the file given is a PCAP, it will first be run through Joy
        in order to generate the necessary JSON output for use here.
        :return:
        """
        if self.file_name is sys.stdin:
            self.f = self.file_name
        else:
            if self.pcap_loader.is_pcap():
                # Run Joy to generate some JSON for use in this script.
                self.pcap_loader.run()
                # Open the json file that was just made.
                ft = SleuthFileType(self.pcap_loader.temp_json['file'])
                if ft.is_gz():
                    self.f = gzip.open(self.pcap_loader.temp_json['file'], 'r')
                elif ft.is_bz2():
                    self.f = bz2.BZ2File(self.pcap_loader.temp_json['file'], 'r')
                else:
                    self.f = open(self.pcap_loader.temp_json['file'], 'r')
            else:
                ft = SleuthFileType(self.file_name)
                if ft.is_gz():
                    self.f = gzip.open(self.file_name, 'r')
                elif ft.is_bz2():
                    self.f = bz2.BZ2File(self.file_name, 'r')
                else:
                    self.f = open(self.file_name, 'r')


class FlowStitchIterator(DictStreamIterator):
    def __init__(self, source):
        self.source = source
        self.active_flows = collections.OrderedDict()

        for f in source:
            key = (f['sa'], f['da'], f['sp'], f['dp'], f['pr'])
            revkey = (f['da'], f['sa'], f['dp'], f['sp'], f['pr'])
            if key in self.active_flows:
                self.active_flows[key] = self.merge(self.active_flows[key], f)
                pass
            elif revkey in self.active_flows:
                self.active_flows[revkey] = self.merge_reverse(self.active_flows[revkey], f)
                pass
            else:
                self.active_flows[key] = f

        self.flows = iter(self.active_flows.values())

    def next(self):
        return self.flows.next()

    # merge f2 into f1, where both flows are in the same direction, and
    # f1 precedes f2 (f1.ts < f2.ts)
    #
    def merge(self, f1, f2):
        for k, v in f2.items():
            if k not in f1:
                f1[k] = f2[k]
            else:
                if k == 'te':
                    f1[k] = max(f1[k], f2[k])
                elif k == 'ip' or k == 'ib':
                    f1[k] += f2[k]
                elif k == 'op' or k == 'ob':
                    f1[k] += f2[k]
                elif k == 'bd':
                    for i, e in enumerate(f2[k]):
                        f1[k][i] += e
                else:
                    pass
            return f1

    # merge f2 into f1, where f2 is in the reverse direction to f1, and
    # f1 precedes f2 (f1.ts < f2.ts)
    #
    def merge_reverse(self, f1, f2):
        for k, v in f2.items():
            if k not in f1:
                if k == 'op':
                    f1['ip'] += f2[k]
                elif k == 'ob':
                    f1['ib'] += f2[k]
                else:
                    f1[k] = f2[k]
            else:
                if k == 'te':
                    f1[k] = max(f1[k], f2[k])
                elif k == 'ip':
                    f1[k] += f2['ob']
                elif k == 'op':
                    f1[k] += f2['ib']
                elif k == 'bd':
                    for i, e in enumerate(f2[k]):
                        f1[k][i] += e
                else:
                    pass
            return f1


class PcapLoader:
    """
    Helper to operate on PCAP files directly
    """
    def __init__(self, file):
        self.file = file
        self.temp_json = {'file': None, 'created': False}

    def cleanup(self):
        """
        Delete the temporary JSON file that was created.
        :return:
        """
        if self.temp_json['created'] is True:
            try:
                os.remove(self.temp_json['file'])
            except OSError:
                pass

    def is_pcap(self):
        """
        Determine whether a file is pcap.
        :return: True if pcap file, False otherwise
        """
        if self.file.endswith('.pcap'):
            return True
        else:
            # Look inside the file and check for pcap magic number
            if sys.byteorder == 'little':
                magic_number = bytearray.fromhex('d4 c3 b2 a1')
            else:
                magic_number = bytearray.fromhex('a1 b2 c3 d4')

            with open(self.file, 'rb') as f:
                ba = bytearray(f.readline())

                if ba[:4] == magic_number:
                    return True
                else:
                    return False

    def run(self):
        """
        Run Joy with the pcap file as input.
        The json output will then be operated upon in this program (joyq).
        A temporary json file (temp-joyq.json.gz) will be written to the user's "home" directory.
        Use the function cleanup() within this class to delete the file before program exit.
        :return:
        """
        cur_dir = os.path.dirname(__file__)
        temp_json_dir = os.path.expanduser('~')
        temp_json_filename = 'temp-joyq.json.gz'
        self.temp_json['file'] = os.path.join(temp_json_dir, temp_json_filename)

        enabled_features = ['bidir=1', 'http=1', 'tls=1', 'dns=1',
                            'ssh=1', 'ppi=1', 'entropy=1']

        # Construct the commands
        command = ['joy', 'outdir=' + temp_json_dir, 'output=' + temp_json_filename]
        command += enabled_features
        command.append(os.path.join(cur_dir, self.file))

        command_local = copy.deepcopy(command)
        command_local[0] = './joy'

        command_source = copy.deepcopy(command)
        command_source[0] = './bin/joy'

        try:
            subprocess.call(command)
        except OSError as e:
            if e.errno == os.errno.ENOENT:
                # Look within the same directory where joyq lives.
                try:
                    subprocess.call(command_local)
                except OSError as ee:
                    if ee.errno == os.errno.ENOENT:
                        # Look in typical source location
                        try:
                            subprocess.call(command_source)
                        except OSError as eee:
                            if eee.errno == os.errno.ENOENT:
                                print('\033[91m' + 'error: could not locate "joy" executable. exiting.' + '\033[0m')
                                sys.exit(1)
                    else:
                        raise
            else:
                raise

        # Set flag indicating the temporary JSON file was made.
        self.temp_json['created'] = True


# main processing pipeline
#
def pipeline():
    parser = argparse.ArgumentParser(
        description='Interrogate JSON flow data and print out matching flows, selected fields, or stats.'
    )
    parser.add_argument("input", nargs='*', default=sys.stdin,
                        help="Input (json or pcap) file(s).")
    parser.add_argument("--select", dest="selection",
                      help="Select key(s) to output.")
    parser.add_argument("--where", dest="filter",
                      help="Filter flows according to the provided key/value.")
    parser.add_argument("--stitch", dest='stitch', action="store_true",
                      help="Stitch together successive flows separated by active timeouts.")
    parser.add_argument("--split", dest="splitfield",
                        help='Split processing into separate pipeline according to each' +
                             'unique value for the provided keys(s).')
    parser.add_argument("--dist", dest='dist', action="store_true",
                        help="Compute distribution over selected keys(s).")
    parser.add_argument("--pretty", dest='pretty', action="store_true",
                        help="Pretty-print JSON output.")
    parser.add_argument("--sum", dest="sumvars",
                        help="Compute sum over selected element(s).")
    parser.add_argument("--tls_sec", dest='tls_sec', action="store_true",
                        help="Report security level of TLS sessions.")
    parser.add_argument("--sec_policy_file", dest='policy_file',
                        default="policy.json", help="File containing seclevel policy; defaults to policy.json")
    parser.add_argument("--sec_failure_threshold", dest='failure_threshold',
                        default=None, help="Integer defining a custom failure threshold for reporting; defaults to specified value in the policy json")
    parser.add_argument("--sec_unknowns", dest='unknowns',
                        default="report", help="Flag to determine handling of unknowns - ignore or report; defaults to report")
    parser.add_argument("--sec_compliance", nargs="*", dest='compliance',
                        help="List of policies to do a soft check for selected cipher suite compliance against, i.e. fips_140; as defined in compliance.json")

    # Parse command line, and check arguments
    args = parser.parse_args()

    if args.pretty:
        json_indent = 3
    else:
        json_indent = None

    # Set flow processor
    #
    if args.selection is not None:
        fp = DictStreamElementSelectProcessor(args.selection)
    else:
        fp = DictStreamProcessor(indent=json_indent)

    if args.splitfield:
        fp = DictStreamSplitProcessor(fp, args.splitfield)

    # Set post-processor
    #
    if args.dist:
        postproc = DictStreamDistributionProcessor()
    elif args.sumvars:
        postproc = DictStreamSumProcessor(args.sumvars, indent=json_indent)
    else:
        postproc = DictStreamProcessor(indent=json_indent)

    # Process all files, with pre- and post-processing
    #
    fp.pre_process()

    for x in args.input:
        # Load the source data from input
        flow_source = FlowIteratorFromFile(x)

        if args.stitch:
            flow_source = FlowStitchIterator(flow_source)
        if args.filter:
            flow_source = DictStreamFilterIterator(flow_source, SleuthPredicate(args.filter))
        if args.tls_sec:
            flow_source = DictStreamEnrichIterator(flow_source, "tls_sec", enrich_tls, policy_file=args.policy_file, unknowns=args.unknowns, compliance=args.compliance, failure_threshold=args.failure_threshold)

        # process all flows from source
        try:
            for flow in flow_source:
                fp.main_process(flow)
        except KeyboardInterrupt:
            sys.exit()
        except:
            raise

    fp.post_process(postproc)


"""
Script entry point
"""
if __name__ == '__main__':
    pipeline()
